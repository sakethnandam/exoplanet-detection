{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from lightkurve import search_lightcurve\n",
        "from astroquery.ipac.nexsci.nasa_exoplanet_archive import NasaExoplanetArchive\n",
        "\n",
        "#configure the batches\n",
        "BIN_POINTS = 200\n",
        "BATCH_SIZE = 40\n",
        "SAVE_DIR = os.path.join(os.getcwd(), \"planet_batches\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "#getting the planet metadata\n",
        "df = NasaExoplanetArchive.query_criteria(\n",
        "    table=\"pscomppars\",\n",
        "    select=\"pl_name,ra,dec,pl_orbper,pl_tranmid,pl_rade,pl_bmasse\",\n",
        "    where=\"pl_orbper is not null and (pl_rade is not null or pl_bmasse is not null)\"\n",
        ").to_pandas()\n",
        "\n",
        "def assign_class(row):\n",
        "    r = row.get(\"pl_rade\", np.nan)\n",
        "    if np.isnan(r):\n",
        "        return None\n",
        "    #earth-like\n",
        "    if r < 1.25:\n",
        "        return 0 \n",
        "    #super-earth\n",
        "    elif r < 2.0:\n",
        "        return 1 \n",
        "    #neptune-like\n",
        "    elif r < 6.0:\n",
        "        return 2 \n",
        "    #jupiter-like\n",
        "    else:\n",
        "        return 3 \n",
        "\n",
        "def fetch_folded_binned_flux(ra, dec, period, t0=None, bin_points=BIN_POINTS, mission_hint=(\"TESS\",\"Kepler\")):\n",
        "    try:\n",
        "        for m in mission_hint:\n",
        "            sr = search_lightcurve(f\"{ra} {dec}\", mission=m)\n",
        "            if len(sr) > 0:\n",
        "                lc_collection = sr\n",
        "                break\n",
        "        else:\n",
        "            return None\n",
        "        lc = lc_collection.download_all().stitch()\n",
        "        lr = lc.normalize().remove_nans()\n",
        "        #flatten\n",
        "        try:\n",
        "            lr = lr.flatten(window_length=401)\n",
        "        except Exception:\n",
        "            pass\n",
        "        folded = lr.fold(period=period, t0=t0) if t0 and np.isfinite(t0) else lr.fold(period=period)\n",
        "        binned = folded.bin(bin_points)\n",
        "        flux = binned.flux.value\n",
        "        if np.any(~np.isfinite(flux)):\n",
        "            return None\n",
        "        return flux\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "#processing the batches\n",
        "for start in range(0, len(df), BATCH_SIZE):\n",
        "    end = min(start + BATCH_SIZE, len(df))\n",
        "    batch_df = df.iloc[start:end]\n",
        "    batch_id = start // BATCH_SIZE\n",
        "    batch_file = os.path.join(SAVE_DIR, f\"batch_{batch_id}.npz\")\n",
        "\n",
        "    if os.path.exists(batch_file):\n",
        "        print(f\"Skipping batch {batch_id} (already processed)\")\n",
        "        continue\n",
        "\n",
        "    Xb, yb = [], []\n",
        "    for _, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Batch {batch_id}\"):\n",
        "        flux = fetch_folded_binned_flux(row[\"ra\"], row[\"dec\"], row[\"pl_orbper\"], t0=row.get(\"pl_tranmid\", None))\n",
        "        lbl = assign_class(row)\n",
        "        if flux is not None and lbl is not None:\n",
        "            Xb.append(flux)\n",
        "            yb.append(lbl)\n",
        "    if len(Xb) > 0:\n",
        "        Xb = np.array(Xb)\n",
        "        yb = np.array(yb)\n",
        "        np.savez(batch_file, X=Xb, y=yb)\n",
        "        print(f\"Saved batch {batch_id} with {len(Xb)} samples\")\n",
        "    else:\n",
        "        print(f\"No valid data in batch {batch_id}\")\n",
        "\n",
        "#merging all the batches into a final dataset since there's too much to process all at once\n",
        "batch_files = sorted([f for f in os.listdir(SAVE_DIR) if f.endswith(\".npz\")])\n",
        "X_list, y_list = [], []\n",
        "for fname in batch_files:\n",
        "    data = np.load(os.path.join(SAVE_DIR, fname))\n",
        "    X_list.append(data[\"X\"])\n",
        "    y_list.append(data[\"y\"])\n",
        "X = np.vstack(X_list)\n",
        "y = np.concatenate(y_list)\n",
        "np.savez(os.path.join(SAVE_DIR, \"final_dataset.npz\"), X=X, y=y)\n",
        "print(\"Final dataset saved:\", X.shape, y.shape)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
